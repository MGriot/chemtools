{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo la classe definitiva per la regressione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "\n",
    "from chemtools.regression import confidence_band\n",
    "from chemtools.regression import prediction_band\n",
    "from chemtools.utility import t_students\n",
    "from chemtools.utility import degrees_of_freedom\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, fit_intercept=True, model='OLS', weights=None, omega=None, X=None, y=None):\n",
    "        self.fit_intercept=intercept\n",
    "        if self.fit_intercept is True: #aggiunge alla matrice delle X una colonna di 1 per l'intercetta\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.coefficients = None\n",
    "        self.intercept=None\n",
    "        self.slope=None\n",
    "        self.model = model\n",
    "        self.weights = weights\n",
    "        self.omega = omega\n",
    "        self.today=datetime.now().strftime(\"%a, %d %b %Y\")\n",
    "        self.hour=datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    def fit(self):\n",
    "        if self.model == 'OLS':\n",
    "            self.coefficients = np.linalg.inv(self.X.T.dot(self.X)).dot(self.X.T).dot(self.y)\n",
    "        elif self.model == 'WLS':\n",
    "            if self.weights is None:\n",
    "                raise ValueError(\"Weights must be provided for WLS model\")\n",
    "            W = np.diag(self.weights)\n",
    "            self.coefficients = np.linalg.inv(self.X.T.dot(W).dot(self.X)).dot(self.X.T).dot(W).dot(self.y)\n",
    "        elif self.model == 'GLS':\n",
    "            if self.omega is None:\n",
    "                raise ValueError(\"Omega matrix must be provided for GLS model\")\n",
    "            Omega_inv = np.linalg.inv(self.omega)\n",
    "            self.coefficients = np.linalg.inv(self.X.T.dot(Omega_inv).dot(self.X)).dot(self.X.T).dot(Omega_inv).dot(self.y)\n",
    "\n",
    "        if self.fit_intercept is True:\n",
    "            self.intercept=self.coefficients[0][0]\n",
    "            self.slope=self.coefficients[1][0]\n",
    "        else:\n",
    "            self.intercept=0\n",
    "            self.slope=self.coefficients[0][0]\n",
    "\n",
    "    def predict(self,X,new_data=True):\n",
    "        if new_data is True and self.fit_intercept is True:\n",
    "            X=np.hstack((np.ones((X.shape[0],1)),X))\n",
    "        return X.dot(self.coefficients)\n",
    "        \n",
    "    \n",
    "    def statistics(self, alpha=0.05):\n",
    "        self.objects_number = self.X.shape[0]\n",
    "        self.object_order = np.arange(1, self.X.shape[0] + 1)\n",
    "        self.x_mean = np.mean(self.X)\n",
    "        self.y_mean = np.mean(self.y)\n",
    "        self.alpha=alpha\n",
    "        self.dof = degrees_of_freedom(self.X) #degrees of freedom\n",
    "        self.y_pred = self.predict(self.X,new_data=False) # Calcola i valori predetti dal modello per la matrice X\n",
    "        self.residuals = self.y - self.y_pred # Calcola i residui come differenza tra i valori osservati e quelli predetti\n",
    "        self.SSxx = np.sum((self.X - self.x_mean) ** 2) # Calcola la somma dei quadrati delle deviazioni di X dalla sua media\n",
    "        self.SSyy = np.sum((self.y - self.y_mean) ** 2) # Calcola la somma dei quadrati delle deviazioni di y dalla sua media\n",
    "        self.SSxy = np.sum((self.X - self.x_mean) * (self.y - self.y_mean)) # Calcola la somma dei prodotti delle deviazioni di X e y dalle loro medie\n",
    "        self.S2x = np.sum(self.X**2) # Calcola la somma dei quadrati dei valori di X\n",
    "        self.S2y = np.sum(self.y**2) # Calcola la somma dei quadrati dei valori di y\n",
    "        self.SSres = np.sum((self.residuals)**2) # Calcola la somma dei quadrati dei residui (somma dei quadrati degli errori)\n",
    "        self.SSexp=((self.y_pred-self.y_mean)**2).sum() # Calcola la somma dei quadrati spiegata dal modello\n",
    "        self.s2=self.SSres/self.dof #varianza residua\n",
    "        self.rse = np.sqrt(self.SSres / self.dof) #root mean square error\n",
    "        self.se2 = (\n",
    "            1\n",
    "            / (self.objects_number * (self.dof))\n",
    "            * (\n",
    "                self.objects_number * self.SSyy\n",
    "                - self.S2y\n",
    "                - self.slope**2 * (self.objects_number * self.SSxx - self.S2x)\n",
    "            )\n",
    "        )\n",
    "        self.Sslope2 = (\n",
    "            self.objects_number\n",
    "            * self.se2\n",
    "            / (self.objects_number * self.SSxx - self.S2x)\n",
    "        )\n",
    "        self.Sintercept2 = self.Sslope2 * (1 / self.objects_number) * self.SSxx\n",
    "        self.k = len(self.coefficients) #number of coefficents that are calculated\n",
    "        self.t_one, self.t_two = t_students(self.alpha, self.dof) #t di students\n",
    "\n",
    "        self.rse = np.sqrt(self.SSres / self.dof) #root mean square error\n",
    "\n",
    "        self.mse = self.SSres / len(self.y) #usa la funzione predict che c'è già\n",
    "\n",
    "        self.rmse = self.mse ** (1 / 2)\n",
    "        self.R2 = self.SSexp / self.SSyy\n",
    "        self.r2 = 1 - (self.SSres / self.SSyy)\n",
    "        self.adjusted_r_squared = (\n",
    "            1\n",
    "            - ((self.objects_number - 1) / (self.objects_number - self.k - 1))\n",
    "            * self.SSres\n",
    "            / self.SSyy\n",
    "        )\n",
    "\n",
    "        # Calcola la varianza residua s2\n",
    "        s2 = self.residuals.T @ self.residuals / (self.X.shape[0] - self.X.shape[1])\n",
    "        # Calcola la matrice di covarianza dei parametri del modello \n",
    "        self.cov_matrix = s2 * np.linalg.inv(np.dot(self.X.T,self.X))\n",
    "        # Calcola l'errore standard dei parametri del modello come radice quadrata della diagonale della matrice di covarianza ( Calcola la deviazione standard dei coefficienti)\n",
    "        self.se_params=np.sqrt(np.diag(self.cov_matrix))        \n",
    "        # Calcola il valore t dei coefficienti come rapporto tra di questi ultimi e il loro errore standard\n",
    "        self.t_params=self.coefficients/self.se_params\n",
    "        # Calcola il p-value dei coefficienti utilizzando la distribuzione t di Student con gradi di libertà pari al numero di osservazioni meno il numero di parametri stimati\n",
    "        self.p_params= 2 * (1 - stats.t.cdf(abs(self.t_params), self.dof))\n",
    "\n",
    "        # Calcola i margini di errore per i coefficients\n",
    "        self.margin_of_error = self.t_two * self.se_params \n",
    "        # Calcola gli intervalli di confidenza al alpha%\n",
    "        self.conf_int_lower = self.coefficients - self.margin_of_error\n",
    "        self.conf_int_upper = self.coefficients + self.margin_of_error\n",
    "        \n",
    "        ## Statistiche sui residui ##\n",
    "        self.residuals_min=format(np.min(self.residuals),\".2e\") # Minimo dei residui\n",
    "        self.residuals_1q=format(np.percentile(self.residuals,25),\".2e\") # Primo quartile dei residui\n",
    "        self.residuals_median=format(np.median(self.residuals),\".2e\") # Mediana dei residui\n",
    "        self.residuals_3q=format(np.percentile(self.residuals,75),\".2e\") # Terzo quartile dei residui\n",
    "        self.residuals_max=format(np.max(self.residuals),\".2e\")\n",
    "        # Calcola il valore di Omnibus e Prob(Omnibus)\n",
    "        self.omnibus, self.prob_omnibus = stats.normaltest(self.residuals)\n",
    "        # Calcola il valore di Skewness e Kurtosis\n",
    "        self.skewness = stats.skew(self.residuals)\n",
    "        self.kurtosis = stats.kurtosis(self.residuals, fisher=False)\n",
    "        # Calcola il valore di Durbin-Watson\n",
    "        self.dw = np.sum(np.diff(self.residuals, axis=0) ** 2) / np.sum(self.residuals ** 2)\n",
    "        # Calcola il valore di Jarque-Bera (JB) e Prob(JB)\n",
    "        self.jb, self.prob_jb = stats.jarque_bera(self.residuals)\n",
    "\n",
    "        # Calcola il valore di Cond. No.\n",
    "        self.cond_no = np.linalg.cond(self.X)\n",
    "        \n",
    "        ## F-statistic\n",
    "        #self.f_statistic=()\n",
    "\n",
    "        ## confidence band\n",
    "        self.CI_Y_upper, self.CI_Y_lower = confidence_band(self.objects_number, self.X, self.x_mean, self.y_pred, self.SSxx, self.t_two)\n",
    "\n",
    "    def plot_residuals(self, library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            plt.scatter(range(len(self.residuals)),self.residuals)\n",
    "            plt.axhline(0,color='r',linestyle='--')\n",
    "            plt.xlabel('Observations')\n",
    "            plt.ylabel('Residuals')\n",
    "        elif library == 'plotly':\n",
    "            fig = px.scatter(x=range(len(self.residuals)), y=self.residuals.ravel())\n",
    "            fig.update_layout(shapes=[dict(type='line', x0=0, x1=len(self.residuals), y0=0, y1=0)],\n",
    "                xaxis_title=\"Observations\",\n",
    "                yaxis_title=\"Residuals\")\n",
    "            fig.show()\n",
    "\n",
    "    def plot_data(self, X, y, library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            plt.scatter(X,y,label='Data')\n",
    "            x_min,x_max=plt.xlim()\n",
    "            x_line=np.linspace(x_min,x_max,num=100)\n",
    "            x_line=x_line.reshape(-1,1)\n",
    "            y_line=self.predict(x_line)\n",
    "            plt.plot(x_line,y_line,'r',label='Regression Line')\n",
    "        elif library == 'plotly':\n",
    "            fig = px.scatter(x=X.ravel(), y=y.ravel())\n",
    "            x_min,x_max=min(X),max(X)\n",
    "            x_line=np.linspace(x_min,x_max,num=100)\n",
    "            x_line=x_line.reshape(-1,1)\n",
    "            y_line=self.predict(x_line)\n",
    "            fig.add_scatter(x=x_line.ravel(),y=y_line.ravel(),mode='lines',name='Regression Line')\n",
    "            fig.show()\n",
    "    \n",
    "    def plot_confidence_band(self,library='matplotlib'):\n",
    "        x_min,x_max=min(self.X[:,1]),max(self.X[:,1])\n",
    "        x_line=np.linspace(x_min,x_max,num=100)[:,None]\n",
    "        y_pred=self.predict(x_line,new_data=True)\n",
    "        se=self.s2*np.sum(np.power(x_line-np.mean(x_line),2),axis=1)[:,None] #errore standard della stima \n",
    "        ci=self.t_two*np.sqrt(se) #intervallo di confidenza \n",
    "        \n",
    "        if library == 'matplotlib':\n",
    "            plt.fill_between(x_line.flatten(),(y_pred-ci).flatten(),(y_pred+ci).flatten(),color='b',alpha=.3,label=\"Confidence Band\")\n",
    "            plt.legend(loc='best')\n",
    "            \n",
    "        elif library == 'plotly':\n",
    "            fig = go.Figure()\n",
    "            fig.add_scatter(x=x_line.ravel(), y=(y_pred-ci).ravel(),mode='lines',line=dict(width=0),showlegend=False)\n",
    "            fig.add_scatter(x=x_line.ravel(), y=(y_pred+ci).ravel(),fill='tonexty',fillcolor='rgba(0,0,255,.3)',mode='lines',line=dict(width=0),name=\"Confidence Band\")\n",
    "            fig.show()\n",
    "        \n",
    "    def plot_prediction_band(self,library='matplotlib'):\n",
    "        x_min,x_max=min(self.X[:,1]),max(self.X[:,1])\n",
    "        x_line=np.linspace(x_min,x_max,num=100)[:,None]\n",
    "        y_pred=self.predict(x_line,new_data=True)\n",
    "        se=self.s2*np.sum(np.power(x_line-np.mean(x_line),2),axis=1)[:,None] #errore standard della stima \n",
    "        pi=self.t_two*np.sqrt(se+self.s2) #intervallo di predizione \n",
    "\n",
    "        if library == 'matplotlib':\n",
    "            plt.fill_between(x_line.flatten(),(y_pred-pi).flatten(),(y_pred+pi).flatten(),color='b',alpha=.3,label=\"Prediction Band\")\n",
    "            plt.legend(loc='best')\n",
    "            \n",
    "        elif library == 'plotly':\n",
    "            fig = go.Figure()\n",
    "            fig.add_scatter(x=x_line.ravel(), y=(y_pred-pi).ravel(),mode='lines',line=dict(width=0),showlegend=False)\n",
    "            fig.add_scatter(x=x_line.ravel(), y=(y_pred+pi).ravel(),fill='tonexty',fillcolor='rgba(0,0,255,.3)',mode='lines',line=dict(width=0),name=\"Prediction Band\")\n",
    "            fig.show()\n",
    "        \n",
    "    def plot_all(self,X,y,alpha=0.05):\n",
    "        fig,ax=plt.figure(figsize=(10,6))\n",
    "        ax=plt.gca()\n",
    "        ax.scatter(X,y,label=\"Data\")\n",
    "        ax.set_xlabel(\"X\",fontsize=14)\n",
    "        ax.set_ylabel(\"Y\",fontsize=14)\n",
    "        ax.set_title(\"Linear Regression with Confidence and Prediction Bands\",fontsize=18)\n",
    "        x_min,x_max=ax.get_xlim()\n",
    "        x_plot=np.linspace(x_min,x_max,num=100)[:,None]    \n",
    "        y_plot=self.predict(x_plot)\n",
    "        ax.plot(x_plot,y_plot,'r',label='Regression Line')\n",
    "        self.plot_confidence_bands(X,y,alpha=alpha)\n",
    "        ax.legend(loc='best',fontsize=14)\n",
    "        \n",
    "    def plot_data_only(self, X, y, library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            plt.scatter(X,y,label='Data')\n",
    "        elif library == 'plotly':\n",
    "            fig = px.scatter(X.ravel(), y.ravel())\n",
    "            fig.show()\n",
    "\n",
    "    def print_summary(self):\n",
    "        headers1 = [\"\",\"Regresssion\",\"result\",\"\"]\n",
    "\n",
    "        table1=[\n",
    "            [\"Dep. Variable:\",\"Y\",\"R-squared:\",f\"{round(self.r2,3)}\"], #riga 1\n",
    "            [\"Model:\",f\"{self.model}\",\"Adj. R-squared:\",f\"{round(self.adjusted_r_squared,3)}\"], #riga 2\n",
    "            [\"Method:\",\"Least Squares\",\"F-statistic:\",\"None\"], #riga 3\n",
    "            [\"Date:\",f\"{self.today}\",\"Prob (F-statistic):\",\"None\"],\n",
    "            [\"Time:\",f\"{self.hour}\",\"Log-Likelihood:\",\"None\"],\n",
    "            [\"No. Observations:\",f\"{self.objects_number}\",\"AIC:\",\"None\"],\n",
    "            [\"Df Residuals:\",f\"{self.dof}\",\"BIC:\",\"None\"],\n",
    "            [\"Df Model:\",f\"{self.k}\",\"\",\"\"],\n",
    "            [\"Covariance Type:\",\"nonrobust\",\"\",\"\"]\n",
    "        ]\n",
    "\n",
    "        print(tabulate(table1, headers=headers1, colalign=(\"left\", \"right\",\"left\", \"right\")))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        headers2 = [\"\",\"Coef\", \"Std Err\", \"t\", \"P>|t|\", \"[0.025\", \"0.975]\"]\n",
    "        table2 = []\n",
    "        for i in range(len(self.coefficients)):\n",
    "            if self.fit_intercept==True:\n",
    "                if i==0:\n",
    "                    row = [f\"Intercept\", f\"{self.coefficients[i][0]}\", f\"{self.se_params[i]}\", f\"{self.t_params[i][0]}\",f\"{self.p_params[i][0]}\",f\"{self.conf_int_lower[i][0]}\",f\"{self.conf_int_upper[i][0]}\"]\n",
    "                else:\n",
    "                    row = [f\"Coefficient {i+1}\", f\"{self.coefficients[i][0]}\", f\"{self.se_params[i]}\", f\"{self.t_params[i][0]}\",f\"{self.p_params[i][0]}\",f\"{self.conf_int_lower[i][0]}\",f\"{self.conf_int_upper[i][0]}\"]\n",
    "            else:\n",
    "                row = [f\"Coefficient {i+1}\", f\"{self.coefficients[i][0]}\", f\"{self.se_params[i]}\", f\"{self.t_params[i][0]}\",f\"{self.p_params[i][0]}\",f\"{self.conf_int_lower[i][0]}\",f\"{self.conf_int_upper[i][0]}\"]\n",
    "            table2.append(row)\n",
    "        print(tabulate(table2, headers=headers2, colalign=(\"left\", \"right\",\"right\",\"right\",\"right\",\"right\",\"right\")))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        headers3 = [\"\",\"\",\"\",\"\"]\n",
    "        table3 = [\n",
    "            [\"Omnibus:\",f\"{self.omnibus[0]}\",\"Durbin-Watson:\",f\"{self.dw}\"], #riga 1\n",
    "            [\"Prob(Omnibus):\",f\"{self.prob_omnibus[0]}\",\"Jarque-Bera (JB):\",f\"{self.jb}\"], #riga 2\n",
    "            [\"Skew:\",f\"{self.skewness[0]}\",\"Prob(JB):\",f\"{self.prob_jb}\"], #riga 3\n",
    "            [\"Kurtosis:\",f\"{self.kurtosis[0]}\",\"Cond. No.\",f\"{self.cond_no}\"]\n",
    "        ]\n",
    "        print(tabulate(table3, headers=headers3, colalign=(\"left\", \"right\",\"left\", \"right\")))\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\")\n",
    "\n",
    "        headers_residuals=[\"Min\",\"1Q\",\"Median\",\"3Q\",\"Max\"]\n",
    "        table_residuals = [[f\"{self.residuals_min}\", f\"{self.residuals_1q}\", f\"{self.residuals_median}\", f\"{self.residuals_3q}\", f\"{self.residuals_max}\"]]\n",
    "        print(\"\\n\")\n",
    "        print(\"Residuals:\")\n",
    "        print(tabulate(table_residuals, headers=headers_residuals, colalign=(\"center\", \"center\",\"center\", \"center\",\"center\")))\n",
    "        print(f\"Residual standard error: {round(self.rse, 2)} on {self.dof} degrees of freedom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "\n",
    "from chemtools.regression import confidence_band\n",
    "from chemtools.regression import prediction_band\n",
    "from chemtools.utility import t_students\n",
    "from chemtools.utility import degrees_of_freedom\n",
    "from chemtools.utility import sort_arrays\n",
    "from chemtools.utility import centered_r_squared, uncentered_r_squared, uncentered_adjusted_r_squared\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, fit_intercept=True, model='OLS', weights=None, omega=None, X=None, y=None):\n",
    "        self.fit_intercept=fit_intercept\n",
    "        self.X_orig, self.y = sort_arrays(X.ravel(), y.ravel())\n",
    "        self.X_orig = np.array([self.X_orig]).T\n",
    "        self.y = np.array([self.y]).T\n",
    "        if self.fit_intercept is True: #aggiunge alla matrice delle X una colonna di 1 per l'intercetta\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), self.X_orig))\n",
    "        self.X=X\n",
    "        self.coefficients = None\n",
    "        self.model = model\n",
    "        self.weights = weights\n",
    "        self.omega = omega\n",
    "        self.today=datetime.now().strftime(\"%a, %d %b %Y\")\n",
    "        self.hour=datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    def fit(self):\n",
    "        if self.model == 'OLS':\n",
    "            self.coefficients = np.linalg.inv(self.X.T.dot(self.X)).dot(self.X.T).dot(self.y)\n",
    "        elif self.model == 'WLS':\n",
    "            if self.weights is None:\n",
    "                raise ValueError(\"Weights must be provided for WLS model\")\n",
    "            W = np.diag(self.weights)\n",
    "            self.coefficients = np.linalg.inv(self.X.T.dot(W).dot(self.X)).dot(self.X.T).dot(W).dot(self.y)\n",
    "        elif self.model == 'GLS':\n",
    "            if self.omega is None:\n",
    "                raise ValueError(\"Omega matrix must be provided for GLS model\")\n",
    "            Omega_inv = np.linalg.inv(self.omega)\n",
    "            self.coefficients = np.linalg.inv(self.X.T.dot(Omega_inv).dot(self.X)).dot(self.X.T).dot(Omega_inv).dot(self.y)\n",
    "\n",
    "    def predict(self,X,new_data=True):\n",
    "        if new_data is True and self.fit_intercept is True:\n",
    "            X=np.hstack((np.ones((X.shape[0],1)),X))\n",
    "        return X.dot(self.coefficients)\n",
    "        \n",
    "    \n",
    "    def statistics(self, alpha=0.05):\n",
    "        self.objects_number = self.X_orig.shape[0]\n",
    "        self.object_order = np.arange(1, self.X_orig.shape[0] + 1)\n",
    "        self.x_mean = np.mean(self.X_orig)\n",
    "        self.y_mean = np.mean(self.y)\n",
    "        self.alpha=alpha\n",
    "        self.dof = degrees_of_freedom(self.X) #degrees of freedom\n",
    "        self.y_pred = self.predict(self.X,new_data=False) # Calcola i valori predetti dal modello per la matrice X\n",
    "        self.residuals = self.y - self.y_pred # Calcola i residui come differenza tra i valori osservati e quelli predetti\n",
    "        self.SSxx = np.sum((self.X_orig - self.x_mean) ** 2) # Calcola la somma dei quadrati delle deviazioni di X dalla sua media\n",
    "        self.SSyy = np.sum((self.y - self.y_mean) ** 2) # Calcola la somma dei quadrati delle deviazioni di y dalla sua media\n",
    "        self.SSxy = np.sum((self.X_orig - self.x_mean) * (self.y - self.y_mean)) # Calcola la somma dei prodotti delle deviazioni di X e y dalle loro medie\n",
    "        self.S2x = np.sum(self.X_orig**2) # Calcola la somma dei quadrati dei valori di X\n",
    "        self.S2y = np.sum(self.y**2) # Calcola la somma dei quadrati dei valori di y\n",
    "        self.SSres = np.sum((self.residuals)**2) # Calcola la somma dei quadrati dei residui (somma dei quadrati degli errori)\n",
    "        self.SSexp=((self.y_pred-self.y_mean)**2).sum() # Calcola la somma dei quadrati spiegata dal modello\n",
    "        self.s2=self.SSres/self.dof #varianza residua\n",
    "        self.rse = np.sqrt(self.SSres / self.dof) #root mean square error\n",
    "        \n",
    "        self.k = len(self.coefficients) #number of coefficents that are calculated\n",
    "        self.t_one, self.t_two = t_students(self.alpha, self.dof) #t di students\n",
    "\n",
    "        self.rse = np.sqrt(self.SSres / self.dof) #root mean square error\n",
    "\n",
    "        self.mse = self.SSres / len(self.y) #usa la funzione predict che c'è già\n",
    "\n",
    "        self.rmse = self.mse ** (1 / 2)\n",
    "        if self.fit_intercept == True:\n",
    "            self.R2 = self.SSexp / self.SSyy\n",
    "            self.r2 = 1 - (self.SSres / self.SSyy)\n",
    "            self.adjusted_r_squared = (\n",
    "                1\n",
    "                - ((self.objects_number - 1) / (self.objects_number - self.k - 1))\n",
    "                * self.SSres\n",
    "                / self.SSyy\n",
    "            )\n",
    "        else:\n",
    "            self.r2=uncentered_r_squared(self.y, self.y_pred)\n",
    "            self.adjusted_r_squared =uncentered_adjusted_r_squared(self.y, self.y_pred,self.k)\n",
    "\n",
    "        # Calcola la varianza residua s2\n",
    "        s2 = self.residuals.T @ self.residuals / (self.X.shape[0] - self.X.shape[1])\n",
    "        # Calcola la matrice di covarianza dei parametri del modello \n",
    "        self.cov_matrix = s2 * np.linalg.inv(np.dot(self.X.T,self.X))\n",
    "        # Calcola l'errore standard dei parametri del modello come radice quadrata della diagonale della matrice di covarianza ( Calcola la deviazione standard dei coefficienti)\n",
    "        self.se_params=np.sqrt(np.diag(self.cov_matrix))        \n",
    "        # Calcola il valore t dei coefficienti come rapporto tra di questi ultimi e il loro errore standard\n",
    "        self.t_params=self.coefficients/self.se_params\n",
    "        # Calcola il p-value dei coefficienti utilizzando la distribuzione t di Student con gradi di libertà pari al numero di osservazioni meno il numero di parametri stimati\n",
    "        self.p_params= 2 * (1 - stats.t.cdf(abs(self.t_params), self.dof))\n",
    "\n",
    "        # Calcola i margini di errore per i coefficients\n",
    "        self.margin_of_error = self.t_two * self.se_params \n",
    "        # Calcola gli intervalli di confidenza al alpha%\n",
    "        self.conf_int_lower = self.coefficients - self.margin_of_error\n",
    "        self.conf_int_upper = self.coefficients + self.margin_of_error\n",
    "        \n",
    "        ## Statistiche sui residui ##\n",
    "        self.residuals_min=format(np.min(self.residuals),\".2e\") # Minimo dei residui\n",
    "        self.residuals_1q=format(np.percentile(self.residuals,25),\".2e\") # Primo quartile dei residui\n",
    "        self.residuals_median=format(np.median(self.residuals),\".2e\") # Mediana dei residui\n",
    "        self.residuals_3q=format(np.percentile(self.residuals,75),\".2e\") # Terzo quartile dei residui\n",
    "        self.residuals_max=format(np.max(self.residuals),\".2e\")\n",
    "        # Calcola il valore di Omnibus e Prob(Omnibus)\n",
    "        self.omnibus, self.prob_omnibus = stats.normaltest(self.residuals)\n",
    "        # Calcola il valore di Skewness e Kurtosis (minimum 8 numbers)\n",
    "        self.skewness = stats.skew(self.residuals)\n",
    "        self.kurtosis = stats.kurtosis(self.residuals, fisher=False)\n",
    "        # Calcola il valore di Durbin-Watson\n",
    "        self.dw = np.sum(np.diff(self.residuals, axis=0) ** 2) / np.sum(self.residuals ** 2)\n",
    "        # Calcola il valore di Jarque-Bera (JB) e Prob(JB)\n",
    "        self.jb, self.prob_jb = stats.jarque_bera(self.residuals)\n",
    "\n",
    "        # Calcola il valore di Cond. No.\n",
    "        self.cond_no = np.linalg.cond(self.X)\n",
    "        \n",
    "        ## F-statistic\n",
    "        #self.f_statistic=()\n",
    "\n",
    "        ## confidence band\n",
    "        self.upper_confidence_band, self.lower_confidence_band = confidence_band(self.objects_number, self.X_orig, self.x_mean, self.y_pred, self.SSxx, self.t_two)\n",
    "        ## Prediction band\n",
    "        self.upper_prediction_band, self.lower_prediction_band = prediction_band(self.objects_number, self.X_orig, self.x_mean, self.y_pred, self.SSxx, self.t_two)\n",
    "\n",
    "    def plot_residuals(self, library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            plt.scatter(range(len(self.residuals)),self.residuals)\n",
    "            plt.axhline(0,color='r',linestyle='--')\n",
    "            plt.xlabel('Observations')\n",
    "            plt.ylabel('Residuals')\n",
    "        elif library == 'plotly':\n",
    "            fig = px.scatter(x=range(len(self.residuals)), y=self.residuals.ravel())\n",
    "            fig.update_layout(shapes=[dict(type='line', x0=0, x1=len(self.residuals), y0=0, y1=0)],\n",
    "                xaxis_title=\"Observations\",\n",
    "                yaxis_title=\"Residuals\")\n",
    "            fig.show()\n",
    "\n",
    "    def plot_data(self, library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            plt.scatter(self.X_orig,self.y,label='Data', c='#000000', alpha=0.25)\n",
    "            plt.legend(loc='best')\n",
    "        elif library == 'plotly':\n",
    "            fig = px.scatter(X.ravel(), y.ravel())\n",
    "            fig.show()\n",
    "\n",
    "    def plot_regression_line(self, library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            x_line=np.linspace(self.X_orig.min(),self.X_orig.max(),num=100)\n",
    "            x_line=x_line.reshape(-1,1)\n",
    "            y_line=self.predict(x_line)\n",
    "            plt.plot(x_line,y_line,'r',label='Regression Line')\n",
    "            # Costruire la stringa dell'equazione di regressione in formato LaTeX\n",
    "            equation_str = r\"$y = \"\n",
    "            if self.fit_intercept:\n",
    "                equation_str += f\"{self.coefficients[0][0]:.2f} + \"\n",
    "                start_idx = 1\n",
    "            else:\n",
    "                start_idx = 0\n",
    "            for i in range(start_idx, len(self.coefficients)):\n",
    "                equation_str += f\"{self.coefficients[i][0]:.2f} x_{{{i - start_idx + 1}}}\"\n",
    "                if i != len(self.coefficients) - 1:\n",
    "                    equation_str += \" + \"\n",
    "            equation_str += r\"$\"\n",
    "            # Aggiungere l'equazione alla legenda del grafico\n",
    "            plt.plot([], [], \" \", label=f'{equation_str}')\n",
    "            plt.legend(loc='best')\n",
    "\n",
    "    \n",
    "    def plot_confidence_band(self,library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            plt.fill_between(self.X_orig.ravel(),self.lower_confidence_band.ravel(),self.upper_confidence_band.ravel(),color='b',alpha=.3,label=\"Confidence Band\")\n",
    "            plt.legend(loc='best')\n",
    "            \n",
    "        \n",
    "    def plot_prediction_band(self,library='matplotlib'):\n",
    "        if library == 'matplotlib':\n",
    "            plt.fill_between(self.X_orig.ravel(),self.lower_prediction_band.ravel(),self.upper_prediction_band.ravel(),color='g',alpha=.3,label=\"Prediction Band\")\n",
    "            plt.legend(loc='best')\n",
    "\n",
    "    def plot_all(self):\n",
    "        self.plot_data()\n",
    "        self.plot_regression_line()\n",
    "        self.plot_confidence_band()\n",
    "        self.plot_prediction_band()\n",
    "\n",
    "    def print_summary(self):\n",
    "        headers1 = [\"\",\"Regresssion\",\"result\",\"\"]\n",
    "\n",
    "        if self.fit_intercept == True:\n",
    "            table1=[\n",
    "                [\"Dep. Variable:\",\"Y\",\"R-squared:\",f\"{round(self.r2,3)}\"], #riga 1\n",
    "                [\"Model:\",f\"{self.model}\",\"Adj. R-squared:\",f\"{round(self.adjusted_r_squared,3)}\"], #riga 2\n",
    "                [\"Method:\",\"Least Squares\",\"F-statistic:\",\"None\"], #riga 3\n",
    "                [\"Date:\",f\"{self.today}\",\"Prob (F-statistic):\",\"None\"],\n",
    "                [\"Time:\",f\"{self.hour}\",\"Log-Likelihood:\",\"None\"],\n",
    "                [\"No. Observations:\",f\"{self.objects_number}\",\"AIC:\",\"None\"],\n",
    "                [\"Df Residuals:\",f\"{self.dof}\",\"BIC:\",\"None\"],\n",
    "                [\"Df Model:\",f\"{self.k}\",\"\",\"\"],\n",
    "                [\"Covariance Type:\",\"nonrobust\",\"\",\"\"]\n",
    "            ]\n",
    "        else:\n",
    "            table1=[\n",
    "                [\"Dep. Variable:\",\"Y\",\"R-squared (uncentered):\",f\"{round(self.r2,3)}\"], #riga 1\n",
    "                [\"Model:\",f\"{self.model}\",\"Adj. R-squared (uncentered):\",f\"{round(self.adjusted_r_squared,3)}\"], #riga 2\n",
    "                [\"Method:\",\"Least Squares\",\"F-statistic:\",\"None\"], #riga 3\n",
    "                [\"Date:\",f\"{self.today}\",\"Prob (F-statistic):\",\"None\"],\n",
    "                [\"Time:\",f\"{self.hour}\",\"Log-Likelihood:\",\"None\"],\n",
    "                [\"No. Observations:\",f\"{self.objects_number}\",\"AIC:\",\"None\"],\n",
    "                [\"Df Residuals:\",f\"{self.dof}\",\"BIC:\",\"None\"],\n",
    "                [\"Df Model:\",f\"{self.k}\",\"\",\"\"],\n",
    "                [\"Covariance Type:\",\"nonrobust\",\"\",\"\"]\n",
    "            ]\n",
    "\n",
    "        print(tabulate(table1, headers=headers1, colalign=(\"left\", \"right\",\"left\", \"right\")))\n",
    "        print(\"\\n\")\n",
    "        if self.fit_intercept == False:\n",
    "            print(\"Notes:\\n [1] R² is computed without centering (uncentered) since the model does not contain a constant.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        headers2 = [\"\",\"Coef\", \"Std Err\", \"t\", \"P>|t|\", \"[0.025\", \"0.975]\"]\n",
    "        table2 = []\n",
    "        for i in range(len(self.coefficients)):\n",
    "            if self.fit_intercept==True:\n",
    "                if i==0:\n",
    "                    row = [f\"Intercept\", f\"{self.coefficients[i][0]}\", f\"{self.se_params[i]}\", f\"{self.t_params[i][0]}\",f\"{self.p_params[i][0]}\",f\"{self.conf_int_lower[i][0]}\",f\"{self.conf_int_upper[i][0]}\"]\n",
    "                else:\n",
    "                    row = [f\"Coefficient {i}\", f\"{self.coefficients[i][0]}\", f\"{self.se_params[i]}\", f\"{self.t_params[i][0]}\",f\"{self.p_params[i][0]}\",f\"{self.conf_int_lower[i][0]}\",f\"{self.conf_int_upper[i][0]}\"]\n",
    "            else:\n",
    "                row = [f\"Coefficient {i+1}\", f\"{self.coefficients[i][0]}\", f\"{self.se_params[i]}\", f\"{self.t_params[i][0]}\",f\"{self.p_params[i][0]}\",f\"{self.conf_int_lower[i][0]}\",f\"{self.conf_int_upper[i][0]}\"]\n",
    "            table2.append(row)\n",
    "        print(tabulate(table2, headers=headers2, colalign=(\"left\", \"right\",\"right\",\"right\",\"right\",\"right\",\"right\")))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        headers3 = [\"\",\"\",\"\",\"\"]\n",
    "        table3 = [\n",
    "            [\"Omnibus:\",f\"{self.omnibus[0]}\",\"Durbin-Watson:\",f\"{self.dw}\"], #riga 1\n",
    "            [\"Prob(Omnibus):\",f\"{self.prob_omnibus[0]}\",\"Jarque-Bera (JB):\",f\"{self.jb}\"], #riga 2\n",
    "            [\"Skew:\",f\"{self.skewness[0]}\",\"Prob(JB):\",f\"{self.prob_jb}\"], #riga 3\n",
    "            [\"Kurtosis:\",f\"{self.kurtosis[0]}\",\"Cond. No.\",f\"{self.cond_no}\"]\n",
    "        ]\n",
    "        print(tabulate(table3, headers=headers3, colalign=(\"left\", \"right\",\"left\", \"right\")))\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\")\n",
    "\n",
    "        headers_residuals=[\"Min\",\"1Q\",\"Median\",\"3Q\",\"Max\"]\n",
    "        table_residuals = [[f\"{self.residuals_min}\", f\"{self.residuals_1q}\", f\"{self.residuals_median}\", f\"{self.residuals_3q}\", f\"{self.residuals_max}\"]]\n",
    "        print(\"\\n\")\n",
    "        print(\"Residuals:\")\n",
    "        print(tabulate(table_residuals, headers=headers_residuals, colalign=(\"center\", \"center\",\"center\", \"center\",\"center\")))\n",
    "        print(f\"Residual standard error: {round(self.rse, 2)} on {self.dof} degrees of freedom\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n=100\n",
    "X = np.random.rand(n, 1)\n",
    "Y = 4 + 3 * X + np.random.randn(n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Regresssion  result\n",
      "-----------------  ----------------  -------------------  -----\n",
      "Dep. Variable:                    Y  R-squared:           0.419\n",
      "Model:                          OLS  Adj. R-squared:      0.407\n",
      "Method:               Least Squares  F-statistic:          None\n",
      "Date:              Fri, 24 Mar 2023  Prob (F-statistic):   None\n",
      "Time:                      09:50:43  Log-Likelihood:       None\n",
      "No. Observations:               100  AIC:                  None\n",
      "Df Residuals:                    98  BIC:                  None\n",
      "Df Model:                         2\n",
      "Covariance Type:          nonrobust\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  Coef    Std Err        t    P>|t|    [0.025    0.975]\n",
      "-------------  -------  ---------  -------  -------  --------  --------\n",
      "Intercept      4.22215   0.193292  21.8434        0   3.83857   4.60573\n",
      "Coefficient 1  2.93694   0.349053  15.1943        0   2.55335   3.32052\n",
      "\n",
      "\n",
      "\n",
      "--------------  ----------  -----------------  --------\n",
      "Omnibus:           11.7462  Durbin-Watson:      1.90115\n",
      "Prob(Omnibus):  0.00281416  Jarque-Bera (JB):   4.09667\n",
      "Skew:             0.137503  Prob(JB):          0.128949\n",
      "Kurtosis:          2.04733  Cond. No.           4.29966\n",
      "\n",
      "\n",
      "Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "\n",
      "Residuals:\n",
      " Min     1Q     Median    3Q     Max\n",
      "-----  ------  --------  -----  -----\n",
      "-1.76  -0.877   0.071    0.735  2.17\n",
      "Residual standard error: 1.01 on 98 degrees of freedom\n"
     ]
    }
   ],
   "source": [
    "test=LinearRegression(fit_intercept=True, X=X, y=Y)\n",
    "test.fit()\n",
    "test.statistics()\n",
    "test.print_summary()\n",
    "#test.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.825\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.823\n",
      "Method:                 Least Squares   F-statistic:                              465.1\n",
      "Date:                Fri, 24 Mar 2023   Prob (F-statistic):                    3.42e-39\n",
      "Time:                        09:47:56   Log-Likelihood:                         -230.00\n",
      "No. Observations:                 100   AIC:                                      462.0\n",
      "Df Residuals:                      99   BIC:                                      464.6\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             9.4466      0.438     21.567      0.000       8.578      10.316\n",
      "==============================================================================\n",
      "Omnibus:                       10.103   Durbin-Watson:                   1.712\n",
      "Prob(Omnibus):                  0.006   Jarque-Bera (JB):                3.903\n",
      "Skew:                          -0.160   Prob(JB):                        0.142\n",
      "Kurtosis:                       2.087   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72b2382ece9768098284d92bbc69d35954e75b60d1e25897d1389c232f4796f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
